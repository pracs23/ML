Reference: Udacity

Gradient descent is a method to optimize your linear models.
Multiple Linear Regression is a technique for when you are comparing more than two variables.
Polynomial Regression for relationships between variables that aren't linear.
Regularization is a technique to assure that your models will not only fit the data available but also extend to new situations.


Glossary
Key Term	Definition
Batch gradient descent	The process of repeatedly calculating errors for all points at the same time and updating weights accordingly.
Error	The vertical distance from a given point to the predictive line.
Feature scaling	Transforming data into a common range of values using standardizing or normalizing.
Gradient descent	The reduction of the error by taking the derivative of the error function with respect to the weights.
L1 Regularization	Absolute values of the coefficients of the model are used for regularization.
L2 Regularization	Squares of the values of the coefficients of the model are used for regularization.
Lambda	The amount by which we punish complex models during the process of regularization.
Learning rate	The amount by which we adjust the weights of our equation. The larger the learning rate, the larger our adjustments.
Mean absolute error	The sum of the absolute value of all errors divided by the total number of points.
Mean squared error	The sum of the square of all errors divided by the total number of points.
Regularization	Taking into consideration the complexity of the model when evaluating regression models.
Stochastic gradient descent	The process of repeatedly calculating errors one point at a time and updating weights accordingly.
